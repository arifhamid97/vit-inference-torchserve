{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchserve Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'transformers[torch]' pillow torchserve torch-model-archiver torchvision nvgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 . Converting hf checkpoint to model.bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification,ViTImageProcessor\n",
    "import torch\n",
    "\n",
    "checkpoint_path = \"funkepal/vit-medicinal-plant-finetune\"\n",
    "save_path = \"./torch_model\"\n",
    "inference_processor = ViTImageProcessor.from_pretrained(checkpoint_path)\n",
    "inference_model = ViTForImageClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# for param_tensor in inference_model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", inference_model.state_dict()[param_tensor].size())\n",
    "\n",
    "torch.save(inference_model.state_dict(),f\"{save_path}/pytorch_model.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Package Your Model Using TorchServe Model Archiver\n",
    "- Use the TorchServe Model Archiver to create a .mar file that includes your model and handler script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating custom handler to handle the VIT process during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## handler.py\n",
    "\n",
    "import logging\n",
    "import io\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "import sys\n",
    "\n",
    "\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, format=\"%(message)s\", level=logging.DEBUG)\n",
    "logger = logging.getLogger(__file__)\n",
    "\n",
    "class ViTHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    Vision Transformer handler class. This handler takes an image as input and returns the classification label.\n",
    "        - example https://github.com/pytorch/serve/tree/master/ts/torch_handler\n",
    "        - performance_guide https://github.com/pytorch/serve/blob/master/docs/performance_guide.md\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ViTHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        # self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"mps\") # Using apple silicon to process our tensor\n",
    "\n",
    "        # Initialize model and processor\n",
    "        self.processor = ViTImageProcessor.from_pretrained(model_dir, local_files_only=True)\n",
    "        self.model = ViTForImageClassification.from_pretrained(model_dir, local_files_only=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        logger.error(f\"Transformer model from path {model_dir} loaded successfully\")\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data:list):\n",
    "        \"\"\"\n",
    "        Preprocess the input data to be suitable for model inference.\n",
    "        \"\"\"\n",
    "        list_of_PIL_images = []\n",
    "        for row in data:\n",
    "            # Compat layer: normally the envelope should just return the data\n",
    "            # directly, but older versions of Torchserve didn't have envelope.\n",
    "            image = row.get(\"data\") or row.get(\"body\") or row.get(\"file\")\n",
    "\n",
    "            # If the image is sent as bytesarray\n",
    "            if isinstance(image, (bytearray, bytes)):\n",
    "\n",
    "                buffer = io.BytesIO(image)\n",
    "                pil_image = Image.open(buffer)\n",
    "                list_of_PIL_images.append(pil_image)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The ViTImageProcessor accept list of PIL Image / Single PIL Image and return: {\"pixel_format\":[[Tensor]]}\n",
    "        \"\"\"\n",
    "        list_of_input_tensor = self.processor(images=list_of_PIL_images, return_tensors=\"pt\").to(self.device)\n",
    "        return list_of_input_tensor\n",
    "\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform inference on the preprocessed data using the loaded model.\n",
    "        \"\"\"\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        return logits.argmax(-1)\n",
    "\n",
    "    def postprocess(self, inference_output:torch.Tensor) -> list:\n",
    "        \"\"\"\n",
    "        Postprocess the inference output to be returned to the client.\n",
    "        \"\"\"\n",
    "        list_of_predicted_class=[]\n",
    "\n",
    "        for idx in range(inference_output.size(0)):\n",
    "            predicted_class = self.model.config.id2label[inference_output[idx].item()]\n",
    "            list_of_predicted_class.append(predicted_class)\n",
    "\n",
    "            logger.info(f\"Model predicted: {predicted_class}\")\n",
    "        \n",
    "        return self._client_to_json_format(list_of_predicted_class)\n",
    "\n",
    "    def _client_to_json_format(self,data:list):\n",
    "        \"\"\"\n",
    "        Helper method to convert respond data to custom format\n",
    "        \"\"\"\n",
    "        final_output = []\n",
    "        for d in data:\n",
    "            final_output.append({'data':d})\n",
    "\n",
    "        return final_output\n",
    "        \n",
    "\n",
    "    def handle(self,data, context) -> list:\n",
    "        \"\"\"\n",
    "        Entry point for TorchServe to handle inference requests. \n",
    "        \n",
    "            - handle function will trigger when torchserve --start\n",
    "            - torchserve handler always accept list as input and list as output\n",
    "            - list(input) -> (preprocess) -> (inference) -> (postprocess) -> list(final output to client)\n",
    "            - for big response tensor that might exceed max_response_size https://github.com/pytorch/serve/issues/2849\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"No of batch process---->{len(data)}\")\n",
    "\n",
    "\n",
    "        if not self.initialized:\n",
    "            self.initialize(context)\n",
    "\n",
    "        if data is None:\n",
    "            return None\n",
    "        \n",
    "        data = self.preprocess(data)\n",
    "        data = self.inference(data)\n",
    "        data = self.postprocess(data)\n",
    "\n",
    "        \"\"\" handler function must return in list type \"\"\"\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compiling the handler.py to mar\n",
    "- See ``model_to_mar.sh`` for more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./model_to_mar.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Start the server\n",
    "- see ``run.sh`` and ``stop.sh``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 . Testing the inference model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Test request to torchserve server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8080/predictions/vision_transformer'\n",
    "files = {'file': open('./sample/1.jpg', 'rb')}\n",
    "\n",
    "response = requests.post(url, files=files)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Batch inference request to torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Define the URL and file path\n",
    "url = 'http://localhost:8080/predictions/vision_transformer'\n",
    "file_path = './sample/1.jpg'  # Replace with your actual file path\n",
    "\n",
    "# Function to send request\n",
    "def send_request(url, file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        files = {'file': file}\n",
    "        response = requests.post(url, files=files)\n",
    "    return response\n",
    "\n",
    "# Number of requests to send\n",
    "N = 100  # Replace with the number of requests you want to send\n",
    "\n",
    "# List to store futures\n",
    "futures = []\n",
    "\n",
    "# Create ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit requests asynchronously\n",
    "    for _ in range(N):\n",
    "        future = executor.submit(send_request, url, file_path)\n",
    "        futures.append(future)\n",
    "    \n",
    "    # Gather results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            response = future.result()\n",
    "            print(f\"Response status: {response.json()}\")\n",
    "            # Process response as needed\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
